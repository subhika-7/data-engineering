# Data Engineering Career  

This repository documents my professional journey in **Data Engineering**. 
It contains structured learning, hands-on projects, and curated resources to demonstrate my skills in building and managing data pipelines, storage, and processing systems.  

ðŸ“Œ Profile  
- **Discipline**: Electronics and Communication Engineering (B.E.)  
- **Focus**: Data Engineering and Cloud Technologies  
- **Interests**: Data Modeling, ETL Development, Cloud Data Solutions, Big Data Processing  
- **Objective**: To develop scalable, reliable, and cost-efficient data systems in enterprise environments.  


ðŸ›  Technical Skills  
- **Programming**: Python, SQL  
- **Databases**: PostgreSQL, MySQL  
- **Data Engineering Tools**: Apache Airflow, Pandas, NumPy  
- **Cloud Platforms**: AWS (Free Tier exploration, focus on S3, RDS, EC2, Lambda)  
- **Big Data Tools**: Kafka, Spark (learning)  
- **Other**: Git, Linux, Docker (beginner)  



ðŸš€ Projects  
- **PostgreSQL Data Modeling** â€“ Designing schemas and optimizing queries for analytics.  
- **ETL Pipeline (Python + Airflow)** â€“ Automated pipeline for extraction, transformation, and loading of structured data.  
- **Streaming with Kafka** â€“ Real-time data ingestion and processing.  
- **Cloud Data Pipeline (AWS)** â€“ Building a serverless ETL pipeline with AWS Lambda, S3, and RDS.  


ðŸ“– Learning Roadmap  
1. Advanced SQL and Data Warehousing  
2. Python for Data Engineering  
3. Workflow Orchestration (Airflow)  
4. Big Data Ecosystem (Spark, Hadoop)  
5. Cloud-based Data Solutions (AWS, GCP, Azure)  
6. Data Lake & Data Warehouse Architectures  

 ðŸ“¬ Contact   
- **Email**: subhika.s88@gmail.com

This repository serves as a **portfolio of my data engineering journey**, showcasing technical skills, applied projects, and continuous learning.  
